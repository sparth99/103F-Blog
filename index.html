<!DOCTYPE html>
<html>
<body>

<h1>CS103F Spring 2019: WE ARE Number 1</h1>
<h2> Members: Parth, Brian, Karl</h2>

<h3> Blog Post 13: April 26, 2019: </h3>


<p>

To resolve the numerous ethical dilemmas faced by fellow computer scientists, we list our Code of Ethics for computer scientists as such:
Respect the work of others and yourself. 
<br>
<br>

Many times, we become so immersed in our work that we become convinced that “our” work is better than the work of “others.” For instance, say Bob and Alice are programmers working on the same project, and while Alice is looking over Bob’s work, she makes suggestions on areas of the code to change. It would be disrespectful and unprofessional for Bob to think the code he writes is “better” and therefore dismiss Alice’s remarks. Our team also hopes that this could also promote a more welcoming work environment: when employees are respective of others and their work, then the environment is not as hostile, which would promote productivity and motivation. In addition, respect between company authorities and employees would avoid power tension, such as the controversial tracking bracelets considered by Amazon. Such overt control of employees is unnecessary in a healthy environment.

<br>
<br>

Report all information and findings accurately, and obtain data ethically. A large aspect of being a computer scientist is working with data, and it is necessary that the results we find are accurately reported. We want to write programs that ultimately benefit others, and that goal cannot be reached if we choose to ignore discrepancies between the results and what we intended to accomplish. For example, Wakefield is such a controversial figure due to the lack of conduct in his experiments and publishing his findings. We can prevent such ethical dilemmas by reporting all data that is true. Even more so, the programs written in the future are heavily based off the programs written today. Therefore, if data is misrepresented, even by a little bit, then other computer scientists would be accustomed to such practice and be susceptible to ethical fading and incrementalism. Furthermore, the data we as computer scientists collect for research is just as important as our findings. Therefore, the method in which data is obtained must be ethical as well. We believe this would discourage the practice of technology companies obtaining information on its users without their explicit consent, as it intrudes on users’ privacy. 


<br>
<br>

Finally, be aware of the intended and unintended consequences of our actions and programs. Applications and extensions of computer science have far reaching implementation in much of our society – it is important that we are vigilant in monitoring the impact of technology like AI in cases where our dependence leads to oversight of groups of people becoming harmed. To not overlook or mishandle such problems, computer scientists need a more general training or education on ethical or social topics, which should help bridge the gap between technical fields and the perhaps less technical areas where the technology will ultimately be used, and also allow programmers to better spot and understand their own biases. This kind of attention to the power of our actions and their consequences is particularly relevant towards the use of computer science to actively enact change. Hacktivism can be highly destructive and is much more ethically condonable when used to voice a message or used in the interest of helping the greater public, like intelligence gathered by the NSA so long as it is conducted securely and with basic transparency. The intent of hacktivism is crucial here as it can be the difference between an attack purely meant to cause harm and something parallel to necessary whistleblowing that hopes to enact positive change. Companies that are able to influence consumer behavior with gamification have the same onus of making sure their techniques aren’t designed to create an entrapping sense of obligation or addiction.

 
<br>
<br>

Our Code of Ethics is designed to aid the individual programmer make good decisions and become more aware of their role of a computer scientist in society, and we hope that it encourages others to take upon their own standard of conduct for themselves.

<br>
<br>
</p>

<img src="IMG_3222.jpeg" alt="Smiley face" height="400" width="575">
<p>Brian, Carl, Parth</p>


<h3> Blog Post 12: April 19, 2019: </h3>


<p>
Gamification is the idea of of using the fun and creativity found in games and applying them to real world applications. While most activities are focused on function and getting the task done quickly, human focused design are built around the fact the humans have emotions, motivations, and different levels of engagement. You can see this being implemented in most large companies and apps. The same sense of progress over time experienced by FarmVille players, in the token of “If you can’t measure it, you can’t improve it,” or more apt towards gamification, “If you can’t keep score, you can’t motivate” (Yu-kai Chou), can also be seen in how the Nike+ app tracks statistics and encourages users to go beat their personal bests or previous records again. A language learning service like Duolingo uses points, badges, and unlockable categories to keep users motivated, and regularly tries to reward progress like by giving updates on a percentage fluency in a language
<br>
<br>
When gamification is applied to mainstream activities of everyday life, it emphasizes the role of intent and motivation to do, or not do, a certain activity. For instance, in the context of games, developers can appeal to the players’ sense of ownership or achievement in order to keep them captivated with the game. This can be exploited to gain the most out of other people, when viewed from a consequentialist framework. This is visualized when casinos utilize gamblers’ sense of winning to keep them addicted, ultimately resulting in a profit for the casino. On the other hand, companies may use similar elements with the intent of making workers fulfill and own their work, as opposed to simply finishing for the sake of finishing
<br>
<br>
Playing incentives and addiction are both successful in the sense that the user is motivated to complete some task. The distinction between the two is when the user is able to finish the task without feeling the need to continue; that is, being able to stop themselves from falling into a downwards spiral. Our team believes that companies can avoid addiction mechanics when implementing gamification in the workplace by having employees respond to a survey representative of the octalysis core drives. The environment can be changed modestly to reflect what employees best respond to, thus promoting a light, motivational workplace. 
<br><br>
</p>

<img src="IMG_3222.jpeg" alt="Smiley face" height="400" width="575">
<p>Brian, Carl, Parth</p>


<h3> Blog Post 11: April 11, 2019: </h3>


<p>
Artificial intelligence is currently used extensively to determine if borrowers will repay a loan, perform risk analysis in the courtroom, and determine recommendations to the public. Its workings are so widely used that it is unlikely that AI will be removed in the future. In the short term, the advantages of AI are clearly visible in society: many hours of manual labor are solved efficiently by a well-designed AI. With time, however, our team believes our extensive reliance on AI makes us less aware of its problems to a point where AI may unwittingly hurt more people than it helps.

<br>
<br>
Different solutions for biases or ethical blind spots in AI address different parts of the problem. Third-party audits have gained some traction where companies hire an auditor to come in and spot problems in code. Code that is less simple or with a complex black-box-ish neural network algorithm will be less readable. This is why there exists the dualfold problem that outside professionals experienced in ethical fields, like doctors or lawyers, lack the necessary technical experience to deal with AI, while programmers are equally less familiar with ethical topics. Broader education in computing like ethics and social courses for computer science students should work to help bridge the gap between technical fields and the humanities and let programmers spot and understand their own biases.

<br>
<br>
As computer scientists, our goal in developing AI is to not only provide efficient solutions or tools, but also to consider for what purposes these tools will be used and who will be affected. It is not enough to focus solely on the consequences of the short run; of course there are major advantages in implementing AI for recommendation services and such. On the other hand, the implicit biases towards certain groups of people, such as COMPAS which incorrectly predicts the risk of black and white defendants in court, are so well hidden it may be impossible to weed out. We suspect that computer scientists are likely to be so driven by the need to provide useful results that they will become susceptible to ethical fading. Current precedents of large-scale ethical dilemmas under companies like Facebook can work to incrementally build up normalization of ethical transgressions in big data or other fields on a similar level to any individual programmer who begins by crossing a small line to gain a little edge on a personal project.
<br><br>
</p>

<img src="IMG_3222.jpeg" alt="Smiley face" height="400" width="575">
<p>Brian, Carl, Parth</p>


<h3> Blog Post 10: April 4, 2019: </h3>


<p>
Hacktivism separates society into the people who were targeted and those who support the cause of the hacks. To the victims of the attacks such as large corporations or national governments, they view the hacking as harmful, as the attack resulted in an unnecessary loss of money and reputation. On the other hand, since such hacking usually becomes frontline news, the cause for which the hackers act becomes broadcasted to the public. Hacktivism hopes to polarize the opinions of the public over the cause, potentially even swaying individuals to join their group of “hackers with a cause”. Our group believes that even though it is beneficial to encourage individuals to act for a cause, it is ultimately more harmful to society when companies and governments are successfully attacked, bringing panic to the public. 
<br>
<br>
Hacktivism is ethical when a message is being voiced, as opposed to causing high financial loss or destruction to a company or government. From a duty-based ethical framework, many hackers, such as those in LulzSec or Anonymous, believe it is their responsibility to bring awareness of several issues to the public. For instance, during the Tunisian Revolution, the group Anonymous voiced their support for the protestors by helping them overcome government firewalls. On the other hand, some hacking attacks can cause direct harm, such as the attack on Sony by LulzSec that costed the company over a hundred million dollars. From a consequentialist framework, more harm resulted from the attack than simply broadcasting a message, and we believe this type of hacktivism is unethical
<br>
<br>
A bystander views actions of a hacktivist group differently, depending on whether or not they agree with the cause of the group. Often times, a hacktivist will always somehow negatively affect another group so it is very subjective. For example, Edward Snowden is a witness to a hacktivist group. We can see the NSA as a type of hacktivist group. They hack into computers and use gathered intelligence to prevent terrorist attacks and more. However, they are still invading other people's privacy. In my perspective, I do not think NSA is at fault. Using hacking for the purpose of protecting the greater good is completely ethical even though some people's’ privacy may be disrupted. However if any of the data is leaked and not secure, I would classify it as unethical. 

<br><br>
</p>

<img src="IMG_3222.jpeg" alt="Smiley face" height="400" width="575">
<p>Brian, Carl, Parth</p>

<h3> Blog Post 9: March 27, 2019: </h3>


<p>
Facebook and other data driven technology companies have recently been under a lot of scrutiny for their use and gathering of data. Facebook collects browsing, financial information, and even creates shadow profiles, profiles for people who do not have an account. Facebook has turned this information to apps, which are the epitome of exploitation. Although collection data can be helpful to providing a better experience for users, it can also cause dangerous breaches, such as the one with Cambridge Analytics. Our group believes Facebook and other tech companies should explicitly explain their data practices. 
<br>
<br>
Data collections and breaches have a fairly negative effect on society. In the past couple years, we have seen many instances in which large tech companies have used the customer information for its own purposes. Users who buy their products unintentionally become products themselves: individuals become targets for ads that bring in more revenue to the companies. With our high dependence on technology, society is ultimately within the grasp of giant monopolies as well. 
<br>
<br>
Tech companies like Facebook provide many free services to users, such as being able to socialize online, in exchange for information on its users. However, Facebook has ignored the duty it holds to its user base by profiteering under a consequence based framework that wants to squeeze out as much exploitable data as possible under a pretense of going unnoticed and unhindered.
<br>
<br>
Collecting data isn’t inherently wrong, but with companies as massive and influential as those in the tech industry are, a certain degree of transparency and information is required to assure that data isn’t being gathered unethically or for dangerous purposes.
Distributing data can make the information more vulnerable to data breaches; government investigation is crucial to monitoring data collection and usage practices. We believe informed consent should be made very explicit and easily understandable by all types of different people. Even as millennials, trying to recall Facebook’s privacy settings felt unclear. When users install the app, all data collection policies should be shown and users should have the ability to easily disable settings that they are not comfortable with. 






<br><br>
</p>

<img src="IMG_3222.jpeg" alt="Smiley face" height="400" width="575">
<p>Brian, Carl, Parth</p>





</body>
</html>